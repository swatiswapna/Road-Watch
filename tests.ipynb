{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Ultralytics YOLO ðŸš€, GPL-3.0 license\n",
    "\n",
    "# import hydra\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.engine.predictor import BasePredictor\n",
    "# from ultralytics import DEFAULT_CONFIG, ROOT, ops\n",
    "from ultralytics.models.utils import ops\n",
    "from ultralytics.utils.checks import check_imgsz\n",
    "from ultralytics.utils.plotting import Annotator, colors, save_one_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "import cv2\n",
    "\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "\n",
    "def perform_ocr_on_image(img,coordinates):\n",
    "    img = cv2.imread(img)\n",
    "    x, y, w, h = map(int, coordinates)\n",
    "    cropped_img = img[y:h, x:w]\n",
    "\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    results = reader.readtext(gray_img)\n",
    "\n",
    "    text = \"\"\n",
    "    for res in results:\n",
    "        if len(results) == 1 or (len(res[1]) > 6 and res[2] > 0.2):\n",
    "            text = res[1]\n",
    "\n",
    "    return str(text)\n",
    "\n",
    "def helmet_detect(img):\n",
    "    global wearing_helmet\n",
    "    wearing_helmet = True\n",
    "    helmet_model = YOLO(\"helmet.pt\")\n",
    "    helmet_pred = helmet_model(img)\n",
    "    list_of_helmets = list(helmet_pred[0].boxes.cls.numpy())\n",
    "    if 1.0 in list_of_helmets:\n",
    "        wearing_helmet = False\n",
    "\n",
    "def detect_read_numberplate(img):\n",
    "    numberplate_model = YOLO(\"best.pt\")\n",
    "    detect_plate = numberplate_model(img,save_crop = True)\n",
    "    coords = list(detect_plate[0].boxes.xyxy.numpy()[0])\n",
    "    text = perform_ocr_on_image(img,coords)\n",
    "    return text\n",
    "\n",
    "\n",
    "def run():\n",
    "    img = r\"C:\\Users\\Dell\\Downloads\\WhatsApp Image 2024-04-10 at 21.44.15 (1).jpeg\"  # Idhar image ka path daalneka\n",
    "    wearing_helmet = True\n",
    "    helmet_detect(img)\n",
    "    text = detect_read_numberplate(img)\n",
    "    print(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ocr_on_image(img, coordinates):\n",
    "    x, y, w, h = map(int, coordinates)\n",
    "    cropped_img = img[y:h, x:w]\n",
    "\n",
    "    gray_img = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2GRAY)\n",
    "    results = reader.readtext(gray_img)\n",
    "\n",
    "    text = \"\"\n",
    "    for res in results:\n",
    "        if len(results) == 1 or (len(res[1]) > 6 and res[2] > 0.2):\n",
    "            text = res[1]\n",
    "\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Dell\\Downloads\\HelmetDetect\\HelmetDetect\\image.PNG: 544x640 1 licence, 132.0ms\n",
      "Speed: 10.0ms preprocess, 132.0ms inference, 9.0ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    }
   ],
   "source": [
    "numberplate_model = YOLO(\"best.pt\")\n",
    "detect_plate = numberplate_model(r\"image.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = detect_plate[0].boxes.xyxy.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(list1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = map(int, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dutud'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_ocr_on_image(\"image.PNG\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     59.998,      275.14,       172.7,      296.78], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('helmet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Dell\\OneDrive\\Pictures\\Screenshots\\Screenshot 2024-04-10 200540.png: 192x416 6 helmets, 1 no_helmet, 118.8ms\n",
      "Speed: 5.0ms preprocess, 118.8ms inference, 2.0ms postprocess per image at shape (1, 3, 192, 416)\n"
     ]
    }
   ],
   "source": [
    "helmetpred = model(r\"C:\\Users\\Dell\\OneDrive\\Pictures\\Screenshots\\Screenshot 2024-04-10 200540.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_helmets = list(helmetpred[0].boxes.cls.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in list_of_helmets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPredictor(BasePredictor):\n",
    "\n",
    "    def get_annotator(self, img):\n",
    "        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        img = torch.from_numpy(img).to(self.model.device)\n",
    "        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\n",
    "        img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        return img\n",
    "\n",
    "    def postprocess(self, preds, img, orig_img):\n",
    "        preds = ops.non_max_suppression(preds,\n",
    "                                        self.args.conf,\n",
    "                                        self.args.iou,\n",
    "                                        agnostic=self.args.agnostic_nms,\n",
    "                                        max_det=self.args.max_det)\n",
    "\n",
    "        for i, pred in enumerate(preds):\n",
    "            shape = orig_img[i].shape if self.webcam else orig_img.shape\n",
    "            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def write_results(self, idx, preds, batch):\n",
    "        p, im, im0 = batch\n",
    "        log_string = \"\"\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "        self.seen += 1\n",
    "        im0 = im0.copy()\n",
    "        if self.webcam:  # batch_size >= 1\n",
    "            log_string += f'{idx}: '\n",
    "            frame = self.dataset.count\n",
    "        else:\n",
    "            frame = getattr(self.dataset, 'frame', 0)\n",
    "\n",
    "        self.data_path = p\n",
    "        # save_path = str(self.save_dir / p.name)  # im.jpg\n",
    "        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')\n",
    "        log_string += '%gx%g ' % im.shape[2:]  # print string\n",
    "        self.annotator = self.get_annotator(im0)\n",
    "\n",
    "        det = preds[idx]\n",
    "        self.all_outputs.append(det)\n",
    "        if len(det) == 0:\n",
    "            return log_string\n",
    "        for c in det[:, 5].unique():\n",
    "            n = (det[:, 5] == c).sum()  # detections per class\n",
    "            log_string += f\"{n} {self.model.names[int(c)]}{'s' * (n > 1)}, \"\n",
    "        # write\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            if self.args.save_txt:  # Write to file\n",
    "                xywh = (ops.xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                line = (cls, *xywh, conf) if self.args.save_conf else (cls, *xywh)  # label format\n",
    "                with open(f'{self.txt_path}.txt', 'a') as f:\n",
    "                    f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "            if self.args.save or self.args.save_crop or self.args.show:  # Add bbox to image\n",
    "                c = int(cls)  # integer class\n",
    "                label = None if self.args.hide_labels else (\n",
    "                    self.model.names[c] if self.args.hide_conf else f'{self.model.names[c]} {conf:.2f}')\n",
    "                \n",
    "                \n",
    "                text_ocr = perform_ocr_on_image(im0,xyxy)\n",
    "                label = text_ocr \n",
    "                \n",
    "                self.annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "            if self.args.save_crop:\n",
    "                imc = im0.copy()\n",
    "                save_one_box(xyxy,\n",
    "                             imc,\n",
    "                             file=self.save_dir / 'crops' / self.model.model.names[c] / f'{self.data_path.stem}.jpg',\n",
    "                             BGR=True)\n",
    "\n",
    "        return log_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.11.5 torch-2.1.0+cpu CPU (11th Gen Intel Core(TM) i5-11260H 2.60GHz)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "model='None' is not a supported model format. See https://docs.ultralytics.com/modes/predict for help.\n\n                   Format     Argument           Suffix    CPU    GPU\n0                 PyTorch            -              .pt   True   True\n1             TorchScript  torchscript     .torchscript   True   True\n2                    ONNX         onnx            .onnx   True   True\n3                OpenVINO     openvino  _openvino_model   True  False\n4                TensorRT       engine          .engine  False   True\n5                  CoreML       coreml       .mlpackage   True  False\n6   TensorFlow SavedModel  saved_model     _saved_model   True   True\n7     TensorFlow GraphDef           pb              .pb   True   True\n8         TensorFlow Lite       tflite          .tflite   True  False\n9     TensorFlow Edge TPU      edgetpu  _edgetpu.tflite   True  False\n10          TensorFlow.js         tfjs       _web_model   True  False\n11           PaddlePaddle       paddle    _paddle_model   True   True\n12                   NCNN         ncnn      _ncnn_model   True   True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def predict(cfg):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     cfg.model = cfg.model or \"yolov8n.pt\" #\"best.pt\"  \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     cfg.imgsz = check_imgsz(cfg.imgsz, min_dim=2)  # check image size\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# cfg.source = cfg.source if cfg.source is not None else ROOT / \"assets\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m predictor \u001b[38;5;241m=\u001b[39m DetectionPredictor()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:216\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Setup model\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_source(source \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msource)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:297\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:366\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Any other format (unsupported)\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_formats\n\u001b[1;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a supported model format. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://docs.ultralytics.com/modes/predict for help.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexport_formats()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# Load external metadata YAML\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metadata, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;129;01mand\u001b[39;00m Path(metadata)\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[1;31mTypeError\u001b[0m: model='None' is not a supported model format. See https://docs.ultralytics.com/modes/predict for help.\n\n                   Format     Argument           Suffix    CPU    GPU\n0                 PyTorch            -              .pt   True   True\n1             TorchScript  torchscript     .torchscript   True   True\n2                    ONNX         onnx            .onnx   True   True\n3                OpenVINO     openvino  _openvino_model   True  False\n4                TensorRT       engine          .engine  False   True\n5                  CoreML       coreml       .mlpackage   True  False\n6   TensorFlow SavedModel  saved_model     _saved_model   True   True\n7     TensorFlow GraphDef           pb              .pb   True   True\n8         TensorFlow Lite       tflite          .tflite   True  False\n9     TensorFlow Edge TPU      edgetpu  _edgetpu.tflite   True  False\n10          TensorFlow.js         tfjs       _web_model   True  False\n11           PaddlePaddle       paddle    _paddle_model   True   True\n12                   NCNN         ncnn      _ncnn_model   True   True"
     ]
    }
   ],
   "source": [
    "# def predict(cfg):\n",
    "#     cfg.model = cfg.model or \"yolov8n.pt\" #\"best.pt\"  \n",
    "#     cfg.imgsz = check_imgsz(cfg.imgsz, min_dim=2)  # check image size\n",
    "    # cfg.source = cfg.source if cfg.source is not None else ROOT / \"assets\"\n",
    "predictor = DetectionPredictor()\n",
    "predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'cfg'"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
